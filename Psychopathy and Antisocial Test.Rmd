---
title: "Psychopathy and Antisocial Test - The Dark Triad"
author: "Bhuvanesh Wadhwani"
date: "2025-01-24"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Calling packages 
```{r Packages, cache = TRUE}

packages_to_use<- c("tidyverse", "dplyr", "mvnormalTest", "lavaan", "lavaangui", "readxl")

for(i in packages_to_use){
  if( ! i %in% rownames(installed.packages())  ) {
    print(paste(i, "not installed; installing now:\n") )
    install.packages(i)
  }
  
  require(i, character.only = TRUE)
}

```

# Take in data
```{r}
data <- read.csv("data.csv")
head(data)
```

# Create synthetic variables
##### According to Jones and Paulhus (2013), psychopathy is a key predictor of reckless antisocial behavior. Hence, in our creation of risk_taking variable, we can assign more weight to psychopathy.
```{r}
set.seed(111)
machiavellianism <- rowSums(data[, 1:9])
narcissism <- rowSums(data[, 10:18])  
psychopathy <- rowSums(data[, 19:27])  

# Create synthetic risk-taking variable
## According to Jones & Paulhus (2013), psyhopathy is a key predictor of reckless antisocial behavior
risk_taking <- 0.5 * psychopathy + 0.3 * narcissism + 0.2 * machiavellianism + rnorm(nrow(data), mean = 0, sd = 1)

# Add to dataset
data$risk_taking <- risk_taking

# Drop country and source
data <- data[, !(names(data) %in% c("country", "source"))]

# Create a new variable 'gender' with random 0 or 1
# 0 = female, 1 = male
set.seed(111)
data$gender <- sample(0:1, nrow(data), replace = TRUE)

head(data)
```


# Check for missing data
```{r}
missing_data <- colSums(is.na(data))
print(missing_data) 

# no missing data
```

# Drop rows
```{r}
# Currently, the dataset is extremely large. Let's select 5000 rows to work with.
set.seed(111)
data <- data[sample(nrow(data), 5000), ]

nrow(data)
```


# Assess normality of variance
```{r}
filtered_data <- data %>%
  select(M1:P9)

mvnout <- mardia(filtered_data)

## Shapiro-Wilk Univariate normality test
mvnout$uv.shapiro

## Mardia Multivariate normality test
mvnout$mv.test


# Results from univariate and multivariate tests indicate that the measures do not come from a normally distributed univariate or multivariate distributions. This will be addressed in model specification stage.
```

# Build SEM model
```{r}
# Define the model
model <- '
# Measurement Model
machiavellianism =~ M1 + M2 + M3 + M4 + M5 + M6 + M7 + M8 + M9
narcissism =~ N1 + N2 + N3 + N4 + N5 + N6 + N7 + N8 + N9
psychopathy =~ P1 + P2 + P3 + P4 + P5 + P6 + P7 + P8 + P9


# Structural Model
risk_taking ~ machiavellianism + narcissism + psychopathy
'
# Fit the SEM model
fit <- sem(model, data = data, estimator = "MLR") # maximum likelihood method

```

# Threshold used
##### CFI: >=0.95 for good fit
##### TLI: >=0.95 for good fit
##### RMSEA: <= .05 close fit, .05 - .08 reasonable fit, >= .10 poor fit
##### SRMR: <=0.08 for good fit

# SEM Results
```{r}
summary(fit, fit.measures = TRUE, standardized = TRUE)

# Chi-square Test (Satorra-Bentler scaled): Significant misfit (p < 0.001). 
# However, the Chi-square test is highly sensitive to large sample sizes and minor model misspecifications. 
# Therefore, we rely on additional fit indices for a comprehensive evaluation.

# Robust CFI: 0.784 (indicates poor fit; acceptable threshold is typically ≥ 0.90).

# Robust TLI: 0.763 (indicates poor fit; acceptable threshold is typically ≥ 0.90).

# RMSEA: 0.083 (indicates poor fit; values below 0.06–0.08 are considered acceptable).

# SRMR: 0.065 (indicates poor fit; values below 0.08 are considered acceptable).

# Overall Model Fit: Based on these fit indices, the model demonstrates poor fit to the data.

# Latent Variables: All factor loadings are statistically significant (p < 0.001), 
# suggesting that the observed indicators reliably measure their respective latent constructs.

# Covariances: All relationships between latent variables are statistically significant (p < 0.001), 
# indicating meaningful connections between the constructs in the model.

parameterEstimates(fit, standardized = TRUE, rsquare = TRUE) %>% 
  filter(op == "r2") %>% 
  select(Item = rhs, R2 = est) 

# R2 values are below .50 for some variables, indicating poor relationships with latent variables

```

# Plot SEM model
```{r}
# This code uses lavaangui, which is a relatively new feature. Make sure that R is version 4.1.0 or later to use this.

#plot_lavaan(fit) # This opens an interactive window.

#lavaangui(fit) # This opens an interactive web application.
```

# Check for multicollinearity
```{r}
# covariances among latent variables
inspect(fit, "cor.lv")

# psychopathy and machiavellianism have very highly correlated
```



# Multigroup SEM
##### 0 = Female, 1 = Male
```{r}
fit_configural <- sem(model, data = data, group = "gender")

summary(fit_configural, fit.measures = TRUE, standardized = TRUE)

# CFI: 0.787 poor fit
# TLI: 0.766 poor fit
# RMSEA: 0.088 poor fit
# SRMR: 0.063 poor fit

```


## Metric Invariance
### Test whether factor loadings are equal across groups
```{r}
fit_metric <- sem(model, data = data, group = "gender", group.equal = "loadings")

summary(fit_metric, fit.measures = TRUE, standardized = TRUE)

# CFI: 0.787 poor fit
# TLI: 0.774 poor fit
# RMSEA: 0.087 poor fit
# SRMR: 0.064 poor fit
```


## Scalar Invariance
### Test whether intercepts of observed variables are equal across groups
```{r}
fit_scalar <- sem(model, data = data, group = "gender", group.equal = c("loadings", "intercepts"))

summary(fit_scalar, fit.measures = TRUE, standardized = TRUE)

# CFI: 0.787 poor fit
# TLI: 0.782 poor fit
# RMSEA: 0.085 poor fit
# SRMR: 0.064 poor fit
```


## Strict Invariance
### Test whether residual variances are equal across groups
```{r}
fit_strict <- sem(model, data = data, group = "gender", group.equal = c("loadings", "intercepts", "residuals"))

summary(fit_strict, fit.measures = TRUE, standardized = TRUE)

# CFI: 0.787 acceptable fit
# TLI: 0.790 acceptable fit
# RMSEA: 0.084 acceptable fit
# SRMR: 0.064 good fit
```


## Model Comparison
```{r}
lavTestLRT(fit_configural, fit_metric, fit_scalar, fit_strict)

# fit_configural
# Used as the reference point.
# Constructs are conceptually similar across groups.
# The same factor structure (pattern of factor loadings) holds for both genders.

# fit_metric
# Chi-Square Difference: 22.667, DF Difference: 24, p = 0.5395
# No significant difference from fit_configural; metric invariance holds.
# Factor loadings are equal across groups.
# Allows comparison of relationships (e.g., regressions or covariances) between constructs across genders.

# fit_scalar
# Chi-Square Difference: 23.774, DF Difference: 25, p = 0.5325
# No significant difference from fit_metric; scalar invariance holds.
# Factor loadings and intercepts are equivalent across groups.
# Any differences in latent means (e.g., gender differences in constructs like "psychopathy" or "narcissism") are meaningful and not due to measurement bias.

# fit_strict
# Chi-Square Difference: 26.705, DF Difference: 28, p = 0.5344
# No significant difference from fit_scalar; strict invariance holds.
# Residual variances are equal across groups.
# This supports the strongest form of measurement invariance, indicating no group-specific differences in how consistently the items measure their constructs.

```



